# Data Workflow

This document explains the data workflow for the Schedule Viewer application.

## Overview

The application works with three data sources that are joined together to create the schedule view:

1. **schedule_data.csv** - Schedule shifts from Connecteam API
2. **cloud9_appts.csv** - Appointments from Cloud9 system
3. **job_locations.csv** - Location mapping data

## Workflow

### Step 1: Refresh Connecteam Data

Extract the latest schedule data from Connecteam API:

```bash
npm run refresh-data
```

This creates/updates: `public/data/schedule_data.csv`

### Step 2: Update Cloud9 Appointments

Drop in your latest Cloud9 appointments file:

```bash
# Place your file at:
public/data/cloud9_appts.csv
```

### Step 3: Join the Data

Run the preprocessing script to join all three CSVs:

```bash
npm run join-data
```

This creates: `public/data/joined_schedules.csv`

**Output includes:**
- All schedule information
- Matched appointments (by TC name + date + location)
- Match quality indicators
- Appointment counts
- Concatenated patient names, times, and statuses

### Step 4: Review Data Quality

Open `public/data/joined_schedules.csv` in Excel or your preferred tool to:

✅ **Check for data issues:**
- Schedules without appointments (57.5% currently - is this expected?)
- Unmatched locations (21.5% currently)
- Missing store_guid fields (0% have exact matches currently)
- Patient name inconsistencies
- Date format issues

✅ **Key columns to review:**
- `match_quality` - "exact", "fuzzy", or "unmatched"
- `appointment_count` - Number of appointments for this shift
- `location` vs `appointment_locations` - Do they match?
- `appointment_patients` - Are these the expected patients?

### Step 5: Fix Data Issues

**Option A: Fix source data**
- Update `cloud9_appts.csv` with corrections
- Re-run `npm run join-data`

**Option B: Add location mappings**
- Update `public/data/job_locations.csv` with new address mappings
- Re-run `npm run join-data`

**Option C: Enrich schedule data**
- Add `store_guid`, `store_name`, `store_id` to `schedule_data.csv`
- This will improve match quality from "fuzzy" to "exact"

### Step 6: (Optional) Update App to Use Joined Data

Once you're satisfied with the data quality, you can optionally update the app to load the pre-joined CSV instead of doing joins in the browser. This would be faster and ensure consistency.

## Data Statistics

From the latest run:

```
Total schedules:              4,272
  With appointments:          1,814 (42.5%)
  Without appointments:       2,458 (57.5%)

Match quality:
  Exact (has store_guid):     0 (0.0%)
  Fuzzy (has location/addr):  3,352 (78.5%)
  Unmatched:                  920 (21.5%)
```

## Troubleshooting

### High percentage of "Without appointments"

This could mean:
- TCs are scheduled but don't have appointments yet (future dates)
- Data sync timing issue (appointments not yet in Cloud9 export)
- TC names don't match between systems
- Date format differences

**Fix:** Check the `tc_name` and `date` fields match exactly between files.

### High percentage of "Unmatched" locations

This means schedules don't have location or address fields populated.

**Fix:**
1. Check Connecteam data to ensure addresses are being extracted
2. Add more mappings to `job_locations.csv`

### No "Exact" matches (0% with store_guid)

The Connecteam data doesn't include store_guid fields currently.

**Fix:** You'll need to enrich the schedule_data.csv with store identifiers, possibly by:
1. Creating a mapping table of address → store_guid
2. Adding this enrichment step to the `join_data.py` script

## Files

### Input Files
- `public/data/schedule_data.csv` - Generated by `scripts/connecteam_extractor.py`
- `public/data/cloud9_appts.csv` - Manually updated by you
- `public/data/job_locations.csv` - Location reference data

### Output File
- `public/data/joined_schedules.csv` - Pre-joined data for review

### Scripts
- `scripts/connecteam_extractor.py` - Extracts schedule data from API
- `scripts/join_data.py` - Joins all three CSVs with match quality indicators
